{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Disease Prediction \n",
    "\n",
    "In this example we'll work with the famous \"UCI Heart Disease\" dataset. This dataset contains a set of attributes related to patient potentially affected by a cardiovascular disease (CVD). CVD is one of the biggest causes of mortality but it's estimated that up to 90% of CVD may be preventable, an early diagnosis could be essential in most cases and AI can achieve this goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification\n",
    "Classification is one of the most common unsupervised learning tasks. In a lot of ML applications we need a model that is able to distinguish between two classes, therefore a binary classifier.\n",
    "With the help of the most common ML libraries (like sklearn) it's really easy to train a binary classifier, however evaluating the performance it's not so easy. We will explore model training/evaluating and concepts like Confusion Matrix, Precision, Recall etc ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "data = pd.read_csv('../../data/raw/heart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the data we have\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute meaning:\n",
    "\n",
    "1. age: age in years\n",
    "2. sex: sex (1 = male; 0 = female)\n",
    "3.  cp: chest pain type\n",
    "    * Value 1: typical angina\n",
    "    * Value 2: atypical angina\n",
    "    * Value 3: non-anginal pain\n",
    "    * Value 4: asymptomatic\n",
    "4. trestbps: resting blood pressure (in mm Hg on admission to the hospital)\n",
    "5. chol: serum cholestoral in mg/dl\n",
    "6. fbs: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n",
    "7. restecg: resting electrocardiographic results\n",
    "    * Value 0: normal\n",
    "    * Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n",
    "    * Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n",
    "8. thalach: maximum heart rate achieved\n",
    "9. exang: exercise induced angina (1 = yes; 0 = no)\n",
    "10. oldpeak = ST depression induced by exercise relative to rest\n",
    "11. slope: the slope of the peak exercise ST segment\n",
    "    * Value 1: upsloping\n",
    "    * Value 2: flat\n",
    "    * Value 3: downsloping\n",
    "12. ca: number of major vessels (0-3) colored by flourosopy\n",
    "13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n",
    "14. target: target variable, this field refers to the presence of heart disease in the patient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean age is the 54 years, adults are more likely than younger people to suffer from cardiovascular disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the target value\n",
    "data.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little bit dirty dataset <3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test and train split\n",
    "train_set, test_set = train_test_split(data, test_size=0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the numerical histogram\n",
    "train_set.hist(bins=50, figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature have different scale, it's a good idea to perform standard scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline to encode categorical variable using one hot encoding, insert missing values using the median strategy and scale the value \n",
    "cat_attr = [\"sex\", \"cp\", \"fbs\", \"restecg\", \"exang\", \"slope\"]\n",
    "num_attr = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\", \"ca\", \"thal\"]\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"std_scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attr),\n",
    "    (\"cat\", OneHotEncoder(), cat_attr)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_set.drop(\"target\", axis=1)\n",
    "y_train = train_set.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pr = full_pipeline.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a simple binary classifier as baseline, a Stochastic Gradient Descent classifier (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf = SGDClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually a good way to evaluate a model is to use cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(sgd_clf, x_train_pr, y_train, cv=3, scoring=\"accuracy\")\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not a great value. I think it's a better idea to evaluate our model using other tools like the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's like cross-validation but it returns the predictions\n",
    "preds = cross_val_predict(sgd_clf, x_train_pr, y_train, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can plot the confusion matrix\n",
    "cm = confusion_matrix(y_train, preds)\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see precison, recall and f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision: \", precision_score(y_train, preds))\n",
    "print(\"Recall: \", recall_score(y_train, preds))\n",
    "print(\"F1 score: \", f1_score(y_train, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another useful metric is the roc auc score\n",
    "print(\"roc auc score:\", roc_auc_score(y_train, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that it's a better idea to try more powerfull models like a RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "rf_preds = cross_val_predict(rf_clf, x_train_pr, y_train, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_train, rf_preds)\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the metrics\n",
    "print(\"Precision: \", precision_score(y_train, rf_preds))\n",
    "print(\"Recall: \", recall_score(y_train, rf_preds))\n",
    "print(\"F1 score: \", f1_score(y_train, rf_preds))\n",
    "print(\"roc auc score:\", roc_auc_score(y_train, rf_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly better thant the SGD classifier. Let's train the random forest classifier on the full dataset and evaluate on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "forest_clf.fit(x_train_pr, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_set.drop(\"target\", axis=1)\n",
    "y_test = test_set.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_pr = full_pipeline.transform(x_test)\n",
    "final_preds = forest_clf.predict(x_test_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the final metrics\n",
    "print(\"Precision: \", precision_score(y_test, final_preds))\n",
    "print(\"Recall: \", recall_score(y_test, final_preds))\n",
    "print(\"F1 score: \", f1_score(y_test, final_preds))\n",
    "print(\"roc auc score:\", roc_auc_score(y_test, final_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a simple example on how to evaluate a classifier, however the result is good! Cross-validation is a good method for model evaluation, but since we split our dataset in 3 folds the model had few data for achieve good performances. With the full dataset we reach a good result!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
